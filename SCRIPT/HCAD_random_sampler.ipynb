{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f839a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import urllib3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "045741da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory\n",
    "output_dir = \"OUTPUT/HCAD_SAMPLES\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "932764dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for parcel endpoint\n",
    "url = \"https://arcweb.hcad.org/server/rest/services/public/public_query/MapServer/0/query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21690cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metro service area polygon (as GeoSeries or single unified geometry)\n",
    "metro_gdf = gpd.read_file(\"../REF\\Metro_MTA_Tax_Area.geojson\")\n",
    "metro_union = metro_gdf.union_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2683f547",
   "metadata": {},
   "source": [
    "# Collect Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4faeb709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_sample(metro_union, target_count=1000, batch_size=500, max_attempts=20, output_dir=\"OUTPUT\"):\n",
    "    # 1. Get all parcel object IDs\n",
    "    url = \"https://arcweb.hcad.org/server/rest/services/public/public_query/MapServer/0/query\"\n",
    "    id_params = {\n",
    "        \"where\": \"1=1\",\n",
    "        \"returnIdsOnly\": \"true\",\n",
    "        \"f\": \"json\"\n",
    "    }\n",
    "    id_response = requests.get(url, params=id_params, verify=False)\n",
    "    all_ids = id_response.json().get(\"objectIds\", [])\n",
    "    \n",
    "    if not all_ids:\n",
    "        raise ValueError(\"No ObjectIDs returned from HCAD endpoint.\")\n",
    "    \n",
    "    # 2. Resampling logic\n",
    "    final_records = []\n",
    "    final_geometries = []\n",
    "    used_ids = set()\n",
    "    attempts = 0\n",
    "\n",
    "    def fetch_and_filter(batch_size=500):\n",
    "        remaining_ids = list(set(all_ids) - used_ids)\n",
    "        if not remaining_ids:\n",
    "            return\n",
    "\n",
    "        sample_ids = random.sample(remaining_ids, min(batch_size, len(remaining_ids)))\n",
    "        query_params = {\n",
    "            \"objectIds\": \",\".join(map(str, sample_ids)),\n",
    "            \"outFields\": \"*\",\n",
    "            \"returnGeometry\": \"true\",\n",
    "            \"f\": \"json\",\n",
    "            \"outSR\": \"4326\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=query_params, verify=False)\n",
    "        features = response.json().get(\"features\", [])\n",
    "\n",
    "        for feature in features:\n",
    "            oid = feature.get(\"attributes\", {}).get(\"OBJECTID\")\n",
    "            if oid in used_ids:\n",
    "                continue\n",
    "\n",
    "            rings = feature.get(\"geometry\", {}).get(\"rings\", [])\n",
    "            if rings and isinstance(rings, list) and len(rings[0]) > 2:\n",
    "                try:\n",
    "                    polygon = Polygon(rings[0])\n",
    "                    if polygon.intersects(metro_union):\n",
    "                        final_records.append(feature[\"attributes\"])\n",
    "                        final_geometries.append(polygon)\n",
    "                        used_ids.add(oid)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "    while len(final_records) < target_count and attempts < max_attempts:\n",
    "        fetch_and_filter(batch_size)\n",
    "        attempts += 1\n",
    "        print(f\"Collected {len(final_records)} valid records (attempt {attempts})\")\n",
    "\n",
    "    if not final_geometries:\n",
    "        raise ValueError(\"No valid geometries found in sampling process.\")\n",
    "\n",
    "    final_gdf = gpd.GeoDataFrame(final_records, geometry=final_geometries, crs=\"EPSG:4326\")\n",
    "\n",
    "    # Metadata\n",
    "    metadata = {\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"sampling_batch_size\": batch_size,\n",
    "        \"total_properties_sampled\": len(final_gdf),\n",
    "        \"sampling_attempts\": attempts\n",
    "    }\n",
    "\n",
    "    # Save with metadata\n",
    "    timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"metro_intersecting_parcels_{timestamp_str}.geojson\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    geojson_obj = json.loads(final_gdf.to_json())\n",
    "    geojson_obj[\"metadata\"] = metadata\n",
    "\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(geojson_obj, f)\n",
    "\n",
    "    return final_gdf, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c798967",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = []\n",
    "\n",
    "independent_sample_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62c83558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 418 valid records (attempt 1)\n",
      "Collected 821 valid records (attempt 2)\n",
      "Collected 1236 valid records (attempt 3)\n",
      "Collected 418 valid records (attempt 1)\n",
      "Collected 836 valid records (attempt 2)\n",
      "Collected 1240 valid records (attempt 3)\n",
      "Collected 411 valid records (attempt 1)\n",
      "Collected 824 valid records (attempt 2)\n",
      "Collected 1244 valid records (attempt 3)\n",
      "Collected 418 valid records (attempt 1)\n",
      "Collected 833 valid records (attempt 2)\n",
      "Collected 1259 valid records (attempt 3)\n",
      "Collected 407 valid records (attempt 1)\n",
      "Collected 817 valid records (attempt 2)\n",
      "Collected 1233 valid records (attempt 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    gdf, metadata = run_random_sample(metro_union)\n",
    "    all_samples.append((gdf, metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0787596",
   "metadata": {},
   "source": [
    "# Combine all Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16f9c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where your sample files are saved\n",
    "sample_dir = \"OUTPUT/HCAD_SAMPLES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c2e7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all GeoJSONs that match pattern\n",
    "sample_files = [f for f in os.listdir(sample_dir) if f.endswith(\".geojson\") and f.startswith(\"metro_intersecting_parcels_\")]\n",
    "\n",
    "# Initialize container\n",
    "all_gdfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58fb54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and label each file\n",
    "for file in sample_files:\n",
    "    filepath = os.path.join(sample_dir, file)\n",
    "    gdf = gpd.read_file(filepath)\n",
    "    \n",
    "    # Extract sample_id from filename or assign sequentially\n",
    "    match = re.search(r\"_(\\d{8}_\\d{6})\", file)\n",
    "    sample_id = match.group(1) if match else file\n",
    "\n",
    "    gdf[\"sample_id\"] = sample_id\n",
    "    all_gdfs.append(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f80691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate\n",
    "combined_gdf = gpd.GeoDataFrame(pd.concat(all_gdfs, ignore_index=True), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68f53832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add index column\n",
    "combined_gdf.reset_index(inplace=True)\n",
    "combined_gdf.rename(columns={\"index\": \"parcel_index\"}, inplace=True)\n",
    "\n",
    "# Save for next steps\n",
    "combined_gdf.to_file(\"OUTPUT/combined_random_samples.geojson\", driver=\"GeoJSON\")\n",
    "combined_gdf.to_parquet(\"OUTPUT/combined_random_samples.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea60162",
   "metadata": {},
   "source": [
    "# All Samples Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14571799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium import GeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b56c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[29.76, -95.37], zoom_start=11, tiles=\"cartodbpositron\")\n",
    "\n",
    "for _, row in combined_gdf.iterrows():\n",
    "    sim_geo = gpd.GeoSeries([row.geometry]).__geo_interface__\n",
    "    folium.GeoJson(sim_geo).add_to(m)\n",
    "\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
